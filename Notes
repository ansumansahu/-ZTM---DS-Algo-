Big O:
Big O Notation is the language we use for talking about how long an algorithm takes to run and which algorithm is better than the other when comes to scale, regardless our computers.

So, to fully understand and wrap it all up. When we talk about Big O and scalability of code, we simply mean we grow bigger and bigger with our Elements or inputs how much does the algorithm slow down, the less it slows down or the slower it slows down is the better. So instead of using DateTime() or Date() objects to calculate the time which is not reliable solution to test the efficiency of the code. We could just measure how many Operations the computer must perform. As each Operation takes different time on every computer. So, Big O allows us and concerns us with how many steps it takes in a function.

O(n) ðŸ¡ª Linear Time
O(1) ðŸ¡ª Constant Time

The 4 Rules Of Calculating the Big O:
1. Worst Case
2. Remove Constants
3. Different terms of inputs:
For a function having multiple loops the solution will be O(n) for the first loop and O(m) for the second loop or what ever we want to name it. So, the overall Big O of this function would be O(n + m). This is how you apply the third rule of calculating the Big O which is Different terms for inputs.

For nested loops, Big O Notation is O(n^2) - Quadratic Time if number of elements looped over is same. While if nested loops have different lengths then - O(nm)

4. Drop Non-Dominants
If you have a function that has a loop that has a Big O of O(n) and two nested loops with Big O of O(n^2) then we drop the non-dominant term,which is O(n) in this case and overall Big O of this function would be O(n^2).

Big O will be the foundation that will help us to decide which data structure is going to be best for our case. For example, Arrays as a data structure allows us to access at O(1) constant time, but when it comes to searching things its going to give us linear Time O(n).

How we measure space complexity?
When a program executes it has two ways to remember things in its memory:
1. Heap : The heap is where we store variables that has assigned values.
2. Stack : The stack is where we keep track of our functions calls.
We simply look at the total size relative to the size of the input and see how many new variable or new memory weâ€™re allocating and how much memory is being used.

What causes space complexity?
- Adding Variables 
- Adding Data Structures 
- Function call 
- Memory Allocations

Data Structure:
A data structure is a collection of values, that values can have a relationship among them, and they can have functions apply to them.

(1) ARRAYS:
Arrays are one of the most commonly-used data structures. The elements of an array are stored in contiguous memory locations.

Arrays are of two types : Static and Dynamic
Static arrays have fixed, pre-defined amount of memory that they can use, whereas in dynamic arrays this is flexible.(In Python we only have dynamic arrays)

Some basic operations and their complexities are given below :
Look-up/Accses - O(1)   Push/Pop - O(1)*
Insert - O(n)           Delete - O(n)

(2) Hash Tables:
Hash Tables are data structures which generally provide very fast O(1) lookups, insertions and deletions. In Python, dictionaries are implemented as hash tables.

In general, the lookup, insert and delete operations all are very fast, in the order of O(1). But in some cases, more than one keys can map to the same slot and that increases the time complexity by some margin, although not by a lot in most cases. This is known as a collision.

Linked lists :
These are, as the name suggests, a list which is linked. It consists of nodes which contain data and a pointer to the next node in the list. The list is connected with the help of these pointers. These nodes are scattered in memory, quite like the buckets in a hash table. The node where the list starts is called the head of the list and the node where it ends, or last node, is called the tail of the list.

The average time complexity of some operations invloving linked lists are as follows:
Look-up : O(n)  Insert : O(n)
Delete : O(n)   Append : O(1)
Prepend : O(1)

Pointer:
A pointer is simply a reference to another place is memory or another object/node. Example:
dict1={'a':"alphabet"} 
dict2=dict1
# we are not creating another memory space storing the same thing instead both dict point to the same memory allocation.
dict1["a"]="digit" #changing the value will be reflected for both cases
print(dict1["a"]) #digit
print(dict2["a"]) #digit
